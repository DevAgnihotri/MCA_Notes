# Unit 5 AI

## **1ï¸âƒ£ Pattern Recognition â€” Introduction and Design Principles**

- 2023 â€” Section A (Q1(i)): Define the term pattern with example.  
- 2024 â€” Section B (Q2(e)): Define pattern recognition. What are the components of pattern recognition?  
- 2023 â€” Section C (Q7(a)): Explain design cycle of a pattern recognition system with diagram.  
- 2022 â€” Section B (Q2(e)): Design principles of pattern recognition system. (Also covers PCA & LDA)

### ğŸ”¹ Definition

**Pattern Recognition (PR)** is the process of automatically identifying and classifying patterns or regularities in data using computational methods.
A **pattern** is an *object, signal, or data structure* that can be represented by a set of features.
â¡ï¸ **Example:** In handwriting recognition, the shape of a letter (e.g., â€˜Aâ€™ or â€˜Bâ€™) is the pattern.

---

### ğŸ”¹ Components of a Pattern Recognition System

1. **Sensing / Data Acquisition:** Collecting raw data (e.g., image, sound, text).
2. **Preprocessing:** Cleaning or normalizing data (e.g., noise removal).
3. **Feature Extraction:** Selecting the most relevant features that represent data efficiently.
4. **Classification / Decision Making:** Assigning patterns to specific categories using algorithms (like decision trees, SVMs, neural networks).
5. **Post-processing / Evaluation:** Assessing performance, accuracy, and refinement.

---

### ğŸ”¹ Design Principles of a PR System

1. **Representation of Patterns:** Properly defining feature sets that best describe the patterns.
2. **Feature Selection:** Reducing dimensionality by keeping only useful features.
3. **Model Selection:** Choosing appropriate classification models (statistical, neural, fuzzy).
4. **Training and Learning:** Adjusting parameters based on training data.
5. **Evaluation:** Using metrics such as accuracy, precision, recall, and confusion matrices.

---

### ğŸ”¹ Design Cycle of Pattern Recognition System

A **pattern recognition design cycle** includes:

1. **Data Collection** â†’
2. **Preprocessing** â†’
3. **Feature Extraction** â†’
4. **Model Design** â†’
5. **Training / Learning** â†’
6. **Testing & Validation** â†’
7. **Performance Evaluation**

**Diagram (text-form):**

```
Raw Data â†’ Preprocessing â†’ Feature Extraction â†’ Classification â†’ Output Decision
```

---

## **2ï¸âƒ£ Statistical Pattern Recognition**

- 2024 â€” Section A (Q1(i)): What is Statistical PR?


### ğŸ”¹ Definition

**Statistical Pattern Recognition (SPR)** uses **probability and statistics** to model the patterns and their variations mathematically.
It assumes each pattern class has a **probability distribution**, and the classifier assigns an unknown sample to the class with the **highest probability (Bayes decision rule).**

â¡ï¸ Example: In speech recognition, words are represented as probabilistic models, and the word with the highest likelihood is recognized.

---

### ğŸ”¹ Key Idea

* Each pattern belongs to a class with probability ( P(C_i|x) )
* Decision is made by maximizing this probability:
  [
  \text{Classify } x \text{ to } C_i \text{ if } P(C_i|x) = \max_j P(C_j|x)
  ]
* Models used: Gaussian distribution, NaÃ¯ve Bayes, Hidden Markov Models.

---

## **3ï¸âƒ£ Parameter Estimation Methods â€” PCA & LDA**

- 2022 â€” Section B (Q2(e)): Explain Principal Component Analysis (PCA) and Linear Discriminant Analysis (LDA).  
- 2023 â€” Section C (Q7(b)): Explain need and concept of principal component analysis in pattern recognition process.  
- 2024 â€” Section C (Q7(a)): Define Principal Component Analysis (PCA). Write steps involved in making principal components to do a classification of given data.


### ğŸ”¸ (A) Principal Component Analysis (PCA)

**Concept:**
PCA is a *dimensionality reduction* technique that transforms data into a new coordinate system such that:

* The first principal component captures the maximum variance.
* Subsequent components capture remaining variance, orthogonal to previous ones.

**Steps to Compute PCA:**

1. Standardize the data.
2. Compute covariance matrix of features.
3. Find eigenvalues and eigenvectors.
4. Sort eigenvectors by decreasing eigenvalues.
5. Form principal components by projecting data on top *k* eigenvectors.

**Use in Pattern Recognition:**

* Reduces dimensionality while retaining key features.
* Simplifies classification (e.g., face recognition, image compression).

---

### ğŸ”¸ (B) Linear Discriminant Analysis (LDA)

**Concept:**
LDA finds a **linear combination of features** that best separates two or more classes.
Unlike PCA (which focuses on variance), LDA focuses on **class separability**.

**Steps:**

1. Compute within-class and between-class scatter matrices.
2. Find the transformation that maximizes between-class scatter and minimizes within-class scatter.
3. Project data on the new axis for better classification.

**Mathematical Objective:**
[
W = \arg \max \frac{|S_B|}{|S_W|}
]
where (S_B) = between-class scatter, (S_W) = within-class scatter.

**Use in PR:**

* Enhances class discrimination in recognition tasks like face or object identification.

---

### ğŸ”¹ Difference between PCA and LDA

| Feature                  | PCA                        | LDA                         |
| ------------------------ | -------------------------- | --------------------------- |
| Basis                    | Unsupervised               | Supervised                  |
| Goal                     | Maximize variance          | Maximize class separability |
| Depends on class labels? | No                         | Yes                         |
| Applications             | Compression, visualization | Classification, recognition |

---

### ğŸ”¹ Relationship to Pattern Recognition

Both **PCA and LDA** serve as **feature extraction / dimensionality reduction tools** in the **design cycle** of a PR system before classification.

Perfect â€” youâ€™ve reached the **final section** of your AI theory unit ğŸ‘
Hereâ€™s a **complete, exam-ready explanation** of the topics:
**Classification Techniques, K-Means Clustering, and Support Vector Machine (SVM)** â€” with theory depth and direct alignment to the given questions.

---

## **1ï¸âƒ£ Classification Techniques â€” Nearest Neighbor Rule and Bayes Classifier**

- 2023 â€” Section A (Q1(j)): State nearest neighbor rule.  
- 2022 â€” Section C (Q7(b)): Show how classification is done by kâ€‘nearest neighbors. Construct KNN on given dataset and predict class for X (p1=4, p2=6), k=3.  
- 2024 â€” Section C (Q7(b)): Explain Nearest Neighbor (NN) rule in detail.  
- Bayes classifier: No matching questions found in the provided selection.

### ğŸ”¹ What is Classification?

Classification is the process of assigning a data sample (unknown pattern) to one of the predefined classes based on features.
It is a **supervised learning** method because the model is trained with labeled data.

---

### ğŸ”¸ (A) Nearest Neighbor (NN) Rule

#### â¤ Definition

The **Nearest Neighbor Rule** states that an unknown sample is assigned to the **class of its closest training sample** in the feature space.

**Formula:**
[
\text{Class}(x) = \text{Class of } \min_i |x - x_i|
]
where

* (x_i) = training sample
* (|x - x_i|) = Euclidean distance

---

#### â¤ Working (Example)

Suppose two classes:

* Class A â†’ (1,1), (2,2)
* Class B â†’ (5,5), (6,6)

Unknown point â†’ (3,3)

Nearest neighbor to (3,3) is (2,2) from Class A â†’ So, **(3,3) belongs to Class A**.

---

### ğŸ”¸ (B) K-Nearest Neighbor (KNN)

#### â¤ Concept

Instead of just one neighbor, **KNN** considers the *K closest samples*.
The class with the majority among those K neighbors is assigned.

**Steps of KNN Algorithm:**

1. Choose K (odd number preferred to avoid ties).
2. Compute the distance between the unknown sample and all training samples.
3. Select the K samples with the smallest distances.
4. Assign the class that occurs most frequently among them.

---

#### â¤ Example (Exam-style)

Dataset â†’

| Point | p1 | p2 | Class |
| ----- | -- | -- | ----- |
| A     | 2  | 4  | +     |
| B     | 4  | 2  | +     |
| C     | 4  | 4  | â€“     |
| D     | 6  | 2  | â€“     |

Predict class for **X(4,6)** with **K = 3**.

Compute distances â†’

* A: âˆš[(4â€“2)Â²+(6â€“4)Â²] = âˆš8 = 2.82
* B: âˆš[(4â€“4)Â²+(6â€“2)Â²] = 4
* C: âˆš[(4â€“4)Â²+(6â€“4)Â²] = 2
* D: âˆš[(6â€“4)Â²+(2â€“6)Â²] = âˆš20 = 4.47

â†’ Closest 3: C(â€“), A(+), B(+) â†’ 2 positives, 1 negative â†’ Class = **Positive (+)**.

---

#### â¤ Advantages

* Simple and effective.
* No training phase (lazy learner).
* Works well for low-dimensional data.

#### â¤ Limitations

* Slow for large datasets.
* Sensitive to noise and irrelevant features.
* Needs proper feature scaling.

---

### ğŸ”¸ (C) Bayes Classifier

#### â¤ Concept

Bayes Classifier is based on **Bayesâ€™ Theorem**:
[
P(C_i|x) = \frac{P(x|C_i)P(C_i)}{P(x)}
]
where

* (P(C_i|x)): posterior probability (class given data)
* (P(x|C_i)): likelihood
* (P(C_i)): prior probability

The classifier assigns (x) to the class with the **highest posterior probability**.

---

#### â¤ NaÃ¯ve Bayes Classifier

Assumes features are **conditionally independent** given the class.
This simplifies computation for multidimensional data.

**Decision Rule:**
[
C = \arg \max_{C_i} P(C_i) \prod_{j} P(x_j|C_i)
]

**Use:** Spam detection, text classification, sentiment analysis.

---

## **2ï¸âƒ£ K-Means Clustering**

- 2023 â€” Section B (Q2(e)): Define clustering. Discuss kâ€‘means clustering algorithm.  
- 2024 â€” Section A (Q1(j)): Explain Kâ€‘means clustering.  
- 2022 â€” Section C (Q7(a)): To which category of clustering schemes does the kâ€‘means algorithm belong? What is its major advantage? Which factors influence computation duration?

### ğŸ”¹ Definition

K-Means is an **unsupervised learning** algorithm used for **clustering**, where data is grouped into *K clusters* based on similarity.

---

### ğŸ”¹ Steps of the Algorithm

1. Choose number of clusters (K).
2. Initialize (K) centroids randomly.
3. Assign each data point to the nearest centroid.
4. Recalculate centroids as the mean of all assigned points.
5. Repeat steps 3â€“4 until centroids no longer move (convergence).

---

### ğŸ”¹ Example

Data points: (2,3), (3,3), (6,6), (7,7), (8,8)
Let (K=2).
Initial centroids â†’ (2,3), (7,7)

Iteration 1:

* Cluster 1 â†’ (2,3), (3,3) â†’ mean = (2.5,3)
* Cluster 2 â†’ (6,6), (7,7), (8,8) â†’ mean = (7,7)

New centroids â†’ (2.5,3), (7,7) â†’ stable â†’ final clusters formed.

---

### ğŸ”¹ Category and Characteristics

* **Category:** Partitional clustering method.
* **Type:** Iterative refinement algorithm.
* **Major Advantage:** Simple, fast, and efficient for large datasets.
* **Factors Influencing Time:**

  * Number of clusters (K).
  * Number of data points (N).
  * Number of iterations for convergence.
  * Dimensionality of data.

---

### ğŸ”¹ Limitations

* Must specify K beforehand.
* Sensitive to initial centroid selection.
* Works only for spherical clusters (not irregular shapes).

---

## **3ï¸âƒ£ Support Vector Machine (SVM)**

- 2022 â€” Section A (Q1(i)): Write short note on Support Vector Machine (SVM).


### ğŸ”¹ Concept

SVM is a **supervised learning algorithm** used for **classification** (and sometimes regression).
It finds the **optimal hyperplane** that separates classes with the **maximum margin**.

---

### ğŸ”¹ Key Idea

For binary classification:
[
w \cdot x + b = 0
]
is the decision boundary (hyperplane).
SVM aims to maximize the margin = distance between the hyperplane and the nearest data points (called **support vectors**).

---

### ğŸ”¹ Diagram (text form)

```
Class + : o o o | Margin |  | Margin | x x x : Class â€“
                <-------> max margin
```

---

### ğŸ”¹ Types

1. **Linear SVM** â€“ when data is linearly separable.
2. **Non-linear SVM** â€“ uses **kernel trick** to project data into higher dimensions:

   * Polynomial kernel
   * Gaussian (RBF) kernel
   * Sigmoid kernel

---

### ğŸ”¹ Advantages

* Works well in high-dimensional spaces.
* Robust to overfitting (especially with regularization).
* Effective for both linear and non-linear data.

### ğŸ”¹ Applications

* Text and image classification
* Bioinformatics (gene classification)
* Handwriting recognition